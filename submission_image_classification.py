# -*- coding: utf-8 -*-
"""Submission_image-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ASLOHVb60e_9oaehoQOr7nu0HQ19KcE

# Proyek Klasifikasi Gambar: PlantVillage Dataset
- **Nama:** M Wildan Nurohman
- **Email:** wildangtg15@gmail.com
- **ID Dicoding:** wildannurohman

## Import Semua Packages/Library yang Digunakan
"""

!pip install tensorflowjs

# cek versi TensorFlow
import tensorflow as tf
print("TensorFlow version:", tf.__version__)

# cek versi Keras
print("Keras version:", tf.keras.__version__)

import os
import random
import shutil
import pathlib

# PIL untuk image processing
from PIL import Image

# matplotlib untuk plotting
import matplotlib.pyplot as plt

# import library TensorFlow dan Keras
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, GlobalAveragePooling2D
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetV2B0, MobileNetV2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Accuracy, Precision, Recall
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing import image

import array as np
from google.colab import drive
import zipfile
from collections import defaultdict
import subprocess

"""## Data Preparation

### Data Loading

dataset didapat dari kaggle, namun untuk mempercepat proses ekstrak saya upload dataset zip tersebut ke gdrive pribadi
"""

# mount gdrive
from google.colab import drive
drive.mount('/content/drive')

# ekstrak file
import zipfile

zip_path = '/content/drive/MyDrive/datasetplantv.zip'
extract_path = '/content/datasetplantvillage'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

def summarize_dataset(directory_path, focus_resolution=None):
    image_per_class = {}
    resolution_stats = defaultdict(int)
    matched_target_res = 0

    for root, dirs, files in os.walk(directory_path):
        if root == directory_path:
            continue  # lewati folder utama

        class_label = os.path.basename(root)
        valid_images = 0

        for fname in files:
            fpath = os.path.join(root, fname)
            try:
                with Image.open(fpath) as img:
                    w, h = img.size
                    key = f"{w}x{h}"
                    resolution_stats[key] += 1
                    valid_images += 1

                    if focus_resolution and key == focus_resolution:
                        matched_target_res += 1
            except Exception:
                continue  # lewati file yang gagal dibuka sebagai gambar

        image_per_class[class_label] = valid_images

    if focus_resolution:
        resolution_stats["matched_target"] = matched_target_res

    return image_per_class, dict(resolution_stats)

# menyesuaikan path dataset
dataset_path = "/content/datasetplantvillage"
wanted_resolution = "256x256"

classes, resolutions = summarize_dataset(dataset_path, wanted_resolution)

print(" Jumlah gambar per kelas:")
for cls, total in classes.items():
    print(f"  {cls}: {total}")

print ("===============================================")

print("\n Statistik resolusi gambar:")
for res, total in resolutions.items():
    label = " (target)" if res == "matched_target" else ""
    print(f"  {res}: {total}{label}")

def random_resize_images(target_dir, size_range=(200, 256)):
    image_count = 0
    for subdir, _, files in os.walk(target_dir):
        for filename in files:
            img_path = os.path.join(subdir, filename)
            try:
                with Image.open(img_path) as im:
                    # menentukan ukuran baru secara acak
                    width = random.choice(range(size_range[0], size_range[1] + 1))
                    height = random.choice(range(size_range[0], size_range[1] + 1))

                    resized = im.resize((width, height), resample=Image.LANCZOS)
                    resized.save(img_path)
                    image_count += 1
            except Exception as e:
                print(f"Skip {img_path}: {e}")

    print(f"‚úîÔ∏è Selesai resize {image_count} gambar di: {target_dir}")

# menjalankan fungsi dengan path dataset PlantVillage
dataset_path = "/content/datasetplantvillage"
random_resize_images(dataset_path)

def analyze_dataset(dataset_path, expected_res=None):
    per_class_count = {}
    resolution_count = defaultdict(int)
    matched_res_count = 0
    total_images = 0

    for subdir, _, files in os.walk(dataset_path):
        if subdir == dataset_path:
            continue  # lewati folder root utama

        class_label = os.path.basename(subdir)
        class_image_count = 0

        for file_name in files:
            img_path = os.path.join(subdir, file_name)
            try:
                with Image.open(img_path) as img:
                    w, h = img.size
                    res_label = f"{w}x{h}"
                    resolution_count[res_label] += 1
                    class_image_count += 1
                    total_images += 1

                    if expected_res and res_label == expected_res:
                        matched_res_count += 1

            except Exception:
                continue  # lewati file rusak atau bukan gambar

        per_class_count[class_label] = class_image_count

    return per_class_count, dict(resolution_count), matched_res_count, total_images

#
dataset_path = "/content/datasetplantvillage"
target_size = "256x256"

classes, resolutions, matched, total = analyze_dataset(dataset_path, target_size)

print(f" Total gambar valid: {total}")
print("\n Jumlah gambar per kelas:")
for cls, count in classes.items():
    print(f"  {cls}: {count}")

print("\nüñºÔ∏è Jumlah gambar per resolusi:")
for res, count in resolutions.items():
    print(f"  {res}: {count}")

print(f"\n Jumlah gambar dengan resolusi target ({target_size}): {matched}")

"""### Data Preprocessing

#### Split Dataset
"""

def summarize_images(dataset_path, focus_class="potato"):
    total_images = 0
    focused_class_images = 0

    for current_dir, _, image_files in os.walk(dataset_path):
        total_images += len(image_files)

        # cek apakah direktori saat ini mengandung kelas target
        if focus_class.lower() in current_dir.lower():
            focused_class_images += len(image_files)

    return total_images, focused_class_images

# ganti ini dengan lokasi folder utama dataset kamu
path_to_dataset = "/content/datasetplantvillage"
class_to_check = "tomato"

# jalankan fungsi
all_images, class_images = summarize_images(path_to_dataset, focus_class=class_to_check)

# cetak hasilnya
print(f" Total images found           : {all_images}")
print(f" Images in class '{class_to_check}' : {class_images}")

def summarize_subclass_counts(dataset_path, focus_class="tomato"):
    subclass_summary = {}

    for current_dir, _, files in os.walk(dataset_path):
        # hanya proses folder yang relevan dengan kelas target
        if focus_class.lower() in current_dir.lower():
            subclass = os.path.basename(current_dir)
            subclass_summary[subclass] = subclass_summary.get(subclass, 0) + len(files)

    return subclass_summary

# tentukan lokasi dataset dan kelas yang ingin dianalisis
dataset_path = "/content/datasetplantvillage"
target_class = "tomato"  # Ganti dengan 'potato' atau lainnya jika perlu

# hitung total gambar per subkelas
summary = summarize_subclass_counts(dataset_path, focus_class=target_class)

# tampilkan hasil
print(f"\n Distribusi subkelas gambar untuk '{target_class}' :\n")
for subclass_name, image_count in summary.items():
    print(f"  ‚Ä¢ {subclass_name:<30} : {image_count} images")

def copy_tomato_folders(dataset_path, target_folder="dataset", target_class="tomato"):
    if not os.path.exists(target_folder):
        os.makedirs(target_folder)

    for root, dirs, files in os.walk(dataset_path):
        for dir_name in dirs:
            if target_class.lower() in dir_name.lower():
                source_path = os.path.join(root, dir_name)
                dest_path = os.path.join(target_folder, dir_name)
                if not os.path.exists(dest_path):
                    shutil.copytree(source_path, dest_path)
                    print(f"Menyalin {source_path} ke {dest_path}")

# sesuaikan path folder utama
dataset_path = "/content/datasetplantvillage"

# ke folder tujuan
target_folder = "/content/dataset-tomato"

copy_tomato_folders(dataset_path, target_folder)

def clean_folder_names(root_path, prefix_to_remove="Tomato___"):
    """
    Menghapus prefix Tomato___ dalam direktori.
    """
    for current_dir, subdirs, _ in os.walk(root_path):
        for subdir in subdirs:
            if subdir.startswith(prefix_to_remove):
                trimmed_name = subdir[len(prefix_to_remove):]
                original_path = os.path.join(current_dir, subdir)
                updated_path = os.path.join(current_dir, trimmed_name)

                os.rename(original_path, updated_path)
                print(f"Folder renamed: {original_path} ‚Üí {updated_path}")

# sesuaikan dengan path
dataset_path = "/content/dataset-tomato"

# Run the renaming process
clean_folder_names(dataset_path)

def show_sample_images(dataset_path):
    """
    menampilkan salah satu gambar random dari setiap kelas.
    """
    sample_images = {}

    for root, _, files in os.walk(dataset_path):
        if files:
            class_name = os.path.basename(root)
            chosen_image = random.choice(files)
            sample_images[class_name] = os.path.join(root, chosen_image)

    num_classes = len(sample_images)
    fig, axes = plt.subplots(1, num_classes, figsize=(4 * num_classes, 5))
    fig.suptitle("Sample Random untuk setiap kelas", fontsize=14)

    # jika hanya satu subplot, jadikan list agar bisa di-iterasi
    if num_classes == 1:
        axes = [axes]

    # menampilkan setiap gambar dengan label kelasnya
    for ax, (class_name, image_path) in zip(axes, sample_images.items()):
        image = Image.open(image_path)
        ax.imshow(image)
        ax.set_title(class_name, fontsize=10)
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# sesuaikan dataset
dataset_path = "/content/dataset-tomato"
show_sample_images(dataset_path)

"""**SPLIT DATASET**

minimal dataset yg dibutuhkan adalah 10.000, jadi akan dipilih 5 kelas saja
"""

def delete_unwanted_folders(dataset_path, keep_folders=['Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato_Bacterial_spot','Tomato_healthy','Tomato_Spider_mites_Two_spotted_spider_mite','Tomato_Septoria_leaf_spot']):
    for item in os.listdir(dataset_path):
        item_path = os.path.join(dataset_path, item)
        if os.path.isdir(item_path) and item not in keep_folders:
            shutil.rmtree(item_path)
            print(f"Menghapus folder: {item_path}")

# sesuaikan path folder
dataset_path = "/content/dataset-tomato"

delete_unwanted_folders(dataset_path)

# membagi dataset dengan rasio 80:20

def split_dataset(dataset_path, train_ratio=0.8):
    # path untuk dataset training dan testing
    train_path = os.path.join(dataset_path, 'train')
    test_path = os.path.join(dataset_path, 'test')

    # membuat folder train dan test
    os.makedirs(train_path, exist_ok=True)
    os.makedirs(test_path, exist_ok=True)

    for root, dirs, files in os.walk(dataset_path):
        if root == dataset_path:
            continue

        class_name = os.path.basename(root)
        if class_name in ['train', 'test']:
            continue

        # membuat folder kelas di dalam train dan test
        train_class_path = os.path.join(train_path, class_name)
        test_class_path = os.path.join(test_path, class_name)
        os.makedirs(train_class_path, exist_ok=True)
        os.makedirs(test_class_path, exist_ok=True)

        # shuffle files
        random.shuffle(files)
        split_index = int(train_ratio * len(files))
        train_files = files[:split_index]
        test_files = files[split_index:]

        # memindahkan file ke folder train
        for file in train_files:
            src_file = os.path.join(root, file)
            dst_file = os.path.join(train_class_path, file)
            shutil.move(src_file, dst_file)

        # memindahkan file ke folder test
        for file in test_files:
            src_file = os.path.join(root, file)
            dst_file = os.path.join(test_class_path, file)
            shutil.move(src_file, dst_file)

# path ke folder utama
dataset_path = "/content/dataset-tomato"

split_dataset(dataset_path)

def delete_directory_unwanted(dataset_path, keep_folders=['train', 'test']):
    for item in os.listdir(dataset_path):
        item_path = os.path.join(dataset_path, item)
        if os.path.isdir(item_path) and item not in keep_folders:
            shutil.rmtree(item_path)
            print(f"Menghapus folder: {item_path}")

# sesuaikan path folder
dataset_path = "/content/dataset-tomato"

delete_directory_unwanted(dataset_path)

"""## Modelling"""

def prepare_image_generator(dataset_path, img_size=(130, 130), batch_size=32):
    train_path = os.path.join(dataset_path, 'train')
    test_path = os.path.join(dataset_path, 'test')

    train_datagen = ImageDataGenerator(
        rescale=1./255,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    # rescale untuk data test
    datagen_test = ImageDataGenerator(rescale=1./255)


    train_generator = train_datagen.flow_from_directory(
        train_path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )

    test_generator = datagen_test.flow_from_directory(
        test_path,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )

    return train_generator, test_generator

# sesuaikan path
dataset_path = "/content/dataset-tomato"

train_generator, test_generator = prepare_image_generator(dataset_path)

# cek kelas yg ada di dataset
class_indices = train_generator.class_indices
print(class_indices)

# menggunakan transfer learning dan MobileNetV2

pretrained_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(150,150,3))

for layer in pretrained_model.layers:
    layer.trainable = False

model = Sequential()

model.add(pretrained_model)

# menambahkan Conv2D and Pooling layers
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten(name="flatten"))
model.add(Dropout(0.5))
model.add(Dense(128, activation="relu"))
model.add(Dense(4, activation='softmax'))

# menggunakan transfer learning based on MobileNetV2

def create_tomato_disease_classifier(input_dimensions=(130, 130, 3), num_classes=5):

   # load pre-trained MobileNetV2
   base_network = MobileNetV2(
       weights="imagenet",
       include_top=False,
       input_shape=input_dimensions
   )

   # freeze model layer supaya tidak perlu melatih ulang
   for layer in base_network.layers:
       layer.trainable = False

   classifier = Sequential(name="TomatoDiseaseClassifier")

   # tambahkan jaringan dasar
   classifier.add(base_network)

   # tambahkan kustom layer untuk feature extraction dan klasifikasi
   classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))
   classifier.add(MaxPooling2D(pool_size=(2, 2)))
   classifier.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))
   classifier.add(MaxPooling2D(pool_size=(2, 2)))

   # ratakan dan tambah layer yg terhubung sepenuhnya
   classifier.add(Flatten(name="flatten_features"))
   classifier.add(Dropout(rate=0.5))
   classifier.add(Dense(128, activation="relu"))
   classifier.add(Dense(num_classes, activation='softmax'))

   return classifier

# buat model
tomato_classifier = create_tomato_disease_classifier()

"""**COMPILE MODEL**"""

# melakukan kompilasi model dengan optimizer Adam, loss categorical cross-entropy, dan metrik evaluasi.

def compile_model(model, learning_rate=0.001):

    # inisialisasi optimizer Adam dengan learning rate yang ditentukan
    optimizer = Adam(learning_rate=learning_rate)

    # compile model dengan konfigurasi yang diinginkan
    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy', Precision(), Recall()]
    )

    return model

# compile model yang telah dibuat sebelumnya
tomato_classifier = compile_model(tomato_classifier)

# tampilkan ringkasan model
tomato_classifier.summary()

# membuat callback early stopping yg memonitor val_accuracy

earlystop_cb = EarlyStopping(
    monitor="val_accuracy",
    patience=5,                   # berhenti jika tidak ada peningkatan dalam 5 epoch
    restore_best_weights=True,
    mode="max",
    verbose=1
)

# membuat callback model checkpoint yg memonitor val_accuracy
checkpoint_cb = ModelCheckpoint(
    "best_model.h5",
    monitor="val_accuracy",
    save_best_only=True,
    mode="max",
    verbose=1
)

# training model
num_epochs = 15

W = tomato_classifier.fit(train_generator,
              epochs=num_epochs,
              validation_data=test_generator,
              callbacks=[earlystop_cb, checkpoint_cb],
              verbose=1)

"""## Evaluasi dan Visualisasi"""

def plot_training_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 4))

    # Grafik Akurasi
    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'r', label='Training Accuracy')
    plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Grafik Loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'r', label='Training Loss')
    plt.plot(epochs, val_loss, 'b', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# sesuaikan fungsi dengan history
plot_training_history(W)

"""## Konversi Model"""

def save_all_formats(model, base_name='model_tomato'):
    # save ke .keras (native keras v3 format)
    keras_path = f'{base_name}.keras'
    model.save(keras_path)
    print(f'[‚úî] Saved as .keras -> {keras_path}')

    # save ke .h5
    h5_path = f'{base_name}.h5'
    model.save(h5_path)
    print(f' Saved as .h5 -> {h5_path}')

    # save ke SavedModel
    saved_model_dir = f'{base_name}_saved_model'
    model.export(saved_model_dir)
    print(f' Saved as SavedModel -> {saved_model_dir}/')

    # konversi ke TFLite
    tflite_path = f'{base_name}.tflite'
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    tflite_model = converter.convert()
    with open(tflite_path, 'wb') as f:
        f.write(tflite_model)
    print(f'Saved as TFLite -> {tflite_path}')

    # konversi ke TensorFlow.js
    tfjs_dir = f'{base_name}_tfjs'
    try:
        subprocess.run([
            'tensorflowjs_converter',
            '--input_format=keras',
            h5_path,
            tfjs_dir
        ], check=True)
        print(f' Converted to TensorFlow.js -> {tfjs_dir}/')
    except Exception as e:
        print(' TFJS conversion failed. Make sure tensorflowjs is installed with: pip install tensorflowjs')
        print(W)

save_all_formats(model, base_name='tomato_classifier')

!pip freeze > requirements.txt